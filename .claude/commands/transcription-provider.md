# Transcription Provider Generator

You are an AI/ML engineer adding speech-to-text providers to AudioBash's TranscriptionService.

## Your Expertise

You understand:
- Audio encoding (WebM from MediaRecorder, WAV from VAD, base64 for APIs)
- The TranscriptionService architecture (550 lines, 6 providers)
- Provider method naming: `transcribeWith{Provider}()`
- The ModelId type union and MODELS array
- Agent mode vs raw mode routing

## Current Architecture

```typescript
// src/services/transcriptionService.ts

export type ModelId =
  | 'gemini-2.0-flash' | 'gemini-2.5-flash'
  | 'openai-whisper' | 'openai-gpt4'
  | 'claude-sonnet' | 'claude-haiku'
  | 'elevenlabs-scribe'
  | 'parakeet-local'
  | 'whisper-local-tiny' | 'whisper-local-base' | 'whisper-local-small';

export const MODELS: ModelInfo[] = [
  { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', provider: 'gemini', description: 'Fast, native audio', supportsAgent: true },
  // ... more models
];
```

## To Add a New Provider

Generate these 4 pieces:

### 1. ModelId Addition

```typescript
// Add to the ModelId type union
| 'newprovider-model'
```

### 2. MODELS Entry

```typescript
// Add to MODELS array
{
  id: 'newprovider-model',
  name: 'NewProvider Model',
  provider: 'newprovider',
  description: 'Brief description',
  supportsAgent: false  // true if LLM-based
},
```

### 3. Provider Method

```typescript
private async transcribeWithNewProvider(
  audioBlob: Blob,
  mode: TranscriptionMode,
  modelId: ModelId,
  durationMs: number
): Promise<TranscribeResult> {
  const apiKey = this.apiKeys.get('newprovider');
  if (!apiKey) {
    throw new Error('NewProvider API key not configured');
  }

  // Convert audio to required format
  const base64Audio = await blobToBase64(audioBlob);
  // OR for FormData APIs:
  // const formData = new FormData();
  // formData.append('audio', audioBlob, 'audio.webm');

  const response = await fetch('https://api.newprovider.com/transcribe', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      audio: base64Audio,
      // Provider-specific options
    }),
  });

  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.message || 'NewProvider API error');
  }

  const data = await response.json();
  const text = data.transcript || data.text || '';
  const cost = this.calculateCost(durationMs, 'newprovider');

  return { text: text.trim(), cost };
}
```

### 4. Router Update

```typescript
// Add case to transcribeAudio switch statement
case 'newprovider':
  return this.transcribeWithNewProvider(audioBlob, mode, modelId, durationMs);
```

### 5. Cost Calculation

```typescript
// Add to calculateCost method rates
'newprovider': 0.006,  // $/minute - check provider pricing
```

## Agent Mode Support

If the provider is an LLM (not just STT):
- Set `supportsAgent: true` in MODELS
- In transcribeWith method, check `mode === 'agent'` and use `buildAgentPrompt()`
- Include terminal context in the prompt

If STT-only (like Whisper):
- Set `supportsAgent: false`
- For agent mode, route through a secondary LLM (pattern: Whisper â†’ GPT-4)

## Settings Integration

Also generate the API key input for Settings.tsx:

```tsx
{/* NewProvider API Key */}
<div className="space-y-2">
  <label className="block text-[10px] text-crt-white/50 font-mono uppercase">
    NewProvider API Key
  </label>
  <input
    type="password"
    value={newproviderKey}
    onChange={(e) => setNewproviderKey(e.target.value)}
    placeholder="np_..."
    className="w-full bg-void-200 border border-void-300 rounded px-3 py-2 text-sm text-crt-white font-mono placeholder-crt-white/20 focus:outline-none focus:border-accent/50"
  />
</div>
```

## Now Generate

Based on the provider name and API documentation the user provides, generate all the code needed to add that provider to AudioBash.
