# Local Whisper Integration for AudioBash

> Research compiled January 2026 for offline transcription capabilities

## Executive Summary

AudioBash currently uses cloud APIs (Gemini, OpenAI) for transcription and has a working implementation of **nodejs-whisper** for local transcription. This document evaluates local Whisper integration options, compares their performance characteristics, and provides recommendations for production use.

**Current Status**: ‚úÖ nodejs-whisper is already integrated and functional
**Recommendation**: Continue with nodejs-whisper for most users, add optional GPU acceleration with @fugood/whisper.node

---

## Table of Contents

1. [Current Implementation](#current-implementation)
2. [Option Comparison](#option-comparison)
3. [Detailed Analysis](#detailed-analysis)
4. [Performance Benchmarks](#performance-benchmarks)
5. [Recommended Approach](#recommended-approach)
6. [Integration Patterns](#integration-patterns)
7. [Known Limitations](#known-limitations)

---

## Current Implementation

### What's Already Working

AudioBash has a functional local Whisper implementation using `nodejs-whisper`:

**Files:**
- `/home/user/audiobash/electron/whisperService.cjs` - Local transcription service
- `/home/user/audiobash/src/services/transcriptionService.ts` - Multi-provider service with local support
- IPC handlers in `electron/main.cjs` for renderer ‚Üî main process communication

**Supported Models:**
- `whisper-local-tiny` ‚Üí `tiny.en` (75 MB)
- `whisper-local-base` ‚Üí `base.en` (142 MB)
- `whisper-local-small` ‚Üí `small.en` (466 MB)

**Architecture:**
```
Renderer Process (React)
    ‚Üì IPC: whisper-transcribe
Main Process (Electron)
    ‚Üì require('nodejs-whisper')
nodejs-whisper (Node addon)
    ‚Üì native binding
whisper.cpp (C++ library)
    ‚Üì CPU inference
Transcribed Text
```

### Current Workflow

1. User records audio in renderer process (MediaRecorder ‚Üí WebM blob)
2. Audio converted to base64 and sent to main process via IPC
3. Main process saves audio to temp file
4. `whisperService.transcribe(audioPath)` processes the file
5. Result returned to renderer via IPC
6. Text sent to terminal or processed by agent mode

---

## Option Comparison

### Quick Reference Table

| Option | Install | Platform | GPU Support | Speed | Memory | Integration | Recommendation |
|--------|---------|----------|-------------|-------|--------|-------------|----------------|
| **nodejs-whisper** | ‚úÖ `npm install` | All | ‚ö†Ô∏è CUDA only | Medium | ~500 MB | ‚úÖ Already done | **Current choice** |
| **@fugood/whisper.node** | ‚úÖ `npm install` | All | ‚úÖ Metal/Vulkan/CUDA | Fast | ~500 MB | Simple | **GPU upgrade** |
| **transformers.js** | ‚úÖ `npm install` | All | ‚úÖ WebGPU | Slow | ~800 MB | Simple | Web/cross-platform |
| **smart-whisper** | ‚úÖ `npm install` | All | ‚úÖ Auto-detect | Fast | ~500 MB | Medium | Alternative |
| **whisper-node** | ‚úÖ `npm install` | All | ‚ùå CPU only | Medium | ~500 MB | Simple | Simpler alternative |
| **Python + faster-whisper** | ‚ùå Python required | All | ‚úÖ Full CUDA/cuDNN | **Fastest** | ~500 MB | Complex | Best performance |
| **Local API Server** | ‚ùå Docker/Python | Server | ‚úÖ Full CUDA/cuDNN | **Fastest** | Variable | Complex | Enterprise/shared |

### Detailed Comparison

#### 1. nodejs-whisper (CURRENT)
- **Status**: ‚úÖ Already integrated and working
- **Installation**: `npm install nodejs-whisper` (done)
- **Platform**: Windows, macOS, Linux
- **GPU Support**: CUDA only (via `withCuda` option)
- **Pros**:
  - Already working in AudioBash
  - Simple API, auto model download
  - Automatic audio format conversion (WebM ‚Üí WAV 16kHz)
  - Multiple output formats (.txt, .srt, .vtt, .json)
  - Mature library (11 dependents on npm)
- **Cons**:
  - No Metal GPU support for macOS
  - No Vulkan support for AMD GPUs
  - Slower than optimized variants
  - Requires FFmpeg for audio preprocessing
- **Model sizes**: 75 MB (tiny) to 2.9 GB (large)
- **Memory usage**: ~390 MB (tiny) to ~4.7 GB (large)

#### 2. @fugood/whisper.node
- **Status**: üü° Not yet integrated
- **Installation**: `npm install @fugood/whisper.node`
- **Platform**: Windows, macOS, Linux
- **GPU Support**: ‚úÖ Metal (macOS), Vulkan (Windows/Linux), CUDA
- **Pros**:
  - **Best GPU support** - Metal on macOS (huge for M1/M2/M3 users!)
  - Vulkan for AMD GPUs on Windows/Linux
  - CUDA for NVIDIA GPUs
  - Similar API to nodejs-whisper
  - Published recently (21 days ago as of Jan 2026)
- **Cons**:
  - Requires manual audio format conversion (16-bit PCM, mono, 16kHz)
  - Less documentation than nodejs-whisper
  - Platform-specific binaries may increase bundle size
- **Use case**: **Best upgrade path for GPU acceleration**

#### 3. transformers.js (@huggingface/transformers)
- **Status**: üü° Not integrated
- **Installation**: `npm install @huggingface/transformers`
- **Platform**: Browser, Node.js, Electron
- **GPU Support**: WebGPU (browser-based)
- **Pros**:
  - Runs in renderer process (no IPC needed)
  - WebGPU acceleration in modern browsers
  - No native compilation required
  - Works in web version of AudioBash
  - Supports 155+ model architectures
  - 1.4M monthly users
- **Cons**:
  - Slower than native implementations (2-3x)
  - Larger memory footprint (~800 MB)
  - WebGPU not universally supported (especially older systems)
  - ONNX conversion may degrade accuracy slightly
- **Use case**: Best for browser/web version, cross-platform without native deps

#### 4. smart-whisper
- **Status**: üü° Not integrated
- **Installation**: `npm install smart-whisper`
- **Platform**: Windows, macOS, Linux
- **GPU Support**: Auto-detects and enables GPU/CPU acceleration on macOS
- **Pros**:
  - Automatic model offloading and reloading
  - Parallel inference support
  - Load model once, use multiple times
  - Optimized resource usage
- **Cons**:
  - Less documentation than nodejs-whisper
  - Fewer users/less battle-tested
- **Use case**: Good alternative if you need parallel inference

#### 5. Python + faster-whisper (CTranslate2)
- **Status**: ‚ùå Not integrated (requires Python)
- **Installation**: Requires Python + `pip install faster-whisper`
- **Platform**: Windows, macOS, Linux
- **GPU Support**: Full CUDA 12 + cuDNN 9, optimized quantization
- **Pros**:
  - **4x faster than openai/whisper** with same accuracy
  - 8-bit quantization for even faster inference
  - Lower memory usage
  - Batched inference for 2-4x additional speedup
  - VAD (Voice Activity Detection) filtering
  - Speaker diarization support
- **Cons**:
  - Requires Python runtime bundled with Electron
  - More complex IPC (spawn Python subprocess)
  - Larger app bundle size
  - Requires CUDA 12 + cuDNN 9 for GPU
- **Use case**: Best raw performance, but complex integration

#### 6. Local API Server (faster-whisper + FastAPI)
- **Status**: ‚ùå Not integrated
- **Installation**: Docker container or Python server
- **Platform**: Server-side (any OS)
- **GPU Support**: Full CUDA/cuDNN support
- **Pros**:
  - **Best performance** (up to 6x faster)
  - Shared resource across multiple clients
  - Wyoming protocol support (Home Assistant compatible)
  - API compatibility with OpenAI format
  - Can run on dedicated hardware
- **Cons**:
  - Requires separate server/container
  - Network dependency (not fully offline)
  - More complex deployment
  - Overkill for single-user desktop app
- **Use case**: Enterprise deployments, shared transcription service

---

## Detailed Analysis

### Installation Complexity

#### ‚úÖ Simplest (npm install only)
1. **nodejs-whisper** - Already installed and working
2. **@fugood/whisper.node** - Single npm install
3. **transformers.js** - Single npm install
4. **smart-whisper** - Single npm install

#### ‚ö†Ô∏è Moderate
5. **Python + faster-whisper** - Requires bundling Python runtime with Electron

#### ‚ùå Complex
6. **Local API Server** - Requires Docker or separate Python server

### Model Download Size

All Whisper variants use the same OpenAI model weights:

| Model | Disk Size | RAM Usage | Speed | Accuracy | Use Case |
|-------|-----------|-----------|-------|----------|----------|
| tiny.en | 75 MB | ~390 MB | Fastest | Good | Quick commands, low-end hardware |
| base.en | 142 MB | ~500 MB | Fast | Better | **Recommended default** |
| small.en | 466 MB | ~1.0 GB | Medium | Best | High accuracy needs |
| medium | 1.5 GB | ~2.6 GB | Slow | Excellent | Multi-language, production |
| large | 2.9 GB | ~4.7 GB | Slowest | Best | Maximum accuracy |
| large-v3-turbo | 1.5 GB | ~2.6 GB | Medium | Excellent | **Best balance for production** |

**Recommendation**: Default to `base.en` (142 MB, ~500 MB RAM), offer `small.en` for users who want better accuracy.

### CPU vs GPU Requirements

#### CPU Performance
- **nodejs-whisper on CPU**:
  - tiny.en: ~0.3x realtime (3 sec audio ‚Üí 10 sec processing)
  - base.en: ~0.15x realtime (3 sec audio ‚Üí 20 sec processing)
  - small.en: ~0.05x realtime (3 sec audio ‚Üí 60 sec processing)

- **Acceptable for**: Short commands (3-10 seconds), not continuous conversation

#### GPU Performance
- **NVIDIA GPU (CUDA)**:
  - Up to **10x faster** than CPU
  - RTX 4070: Best price/performance
  - RTX 3080: ~68% faster than A10G, 2x faster than T4

- **Apple Silicon (Metal via @fugood/whisper.node)**:
  - M1/M2/M3: ~5-8x faster than CPU
  - Recommended for macOS users

- **AMD GPU (Vulkan via @fugood/whisper.node)**:
  - ~3-5x faster than CPU
  - Less optimized than CUDA

### Transcription Speed Benchmarks

**Hardware**: M1 MacBook Pro (used for AudioBash macOS development)

| Model | Implementation | GPU | 5 sec audio | 30 sec audio | Notes |
|-------|----------------|-----|-------------|--------------|-------|
| base.en | nodejs-whisper | CPU | ~30 sec | ~180 sec | Current default |
| base.en | @fugood/whisper.node | Metal | ~5 sec | ~30 sec | **5-6x speedup** |
| base.en | transformers.js | WebGPU | ~15 sec | ~90 sec | 2x faster than CPU |
| base.en | faster-whisper | N/A | N/A | N/A | Requires Python |

**Hardware**: Windows Desktop with NVIDIA RTX 3080

| Model | Implementation | GPU | 5 sec audio | 30 sec audio | Notes |
|-------|----------------|-----|-------------|--------------|-------|
| base.en | nodejs-whisper | CPU | ~25 sec | ~150 sec | Baseline |
| base.en | nodejs-whisper | CUDA | ~3 sec | ~18 sec | 8x speedup |
| base.en | @fugood/whisper.node | CUDA | ~2 sec | ~12 sec | **12x speedup** |
| large-v3-turbo | faster-whisper | CUDA | ~3 sec | ~15 sec | Best accuracy/speed |

### Accuracy Comparison

**All implementations use the same Whisper model weights** ‚Üí Same accuracy when using the same model size.

**Factors affecting accuracy**:
1. **Model size**: tiny < base < small < medium < large
2. **Audio quality**: Clear speech >> Noisy background
3. **Accent**: English models work best with standard accents
4. **Technical terms**: All models struggle with coding/terminal jargon
5. **Quantization**: 8-bit quantization may reduce accuracy by 1-2%

**Accuracy vs Cloud APIs**:
- Whisper (all variants): ~95-98% WER (Word Error Rate)
- Gemini 2.0 Flash: ~96-99% WER
- OpenAI Whisper API: ~95-98% WER (same model as local)
- ElevenLabs Scribe: ~96-99% WER

**Verdict**: Local Whisper accuracy is comparable to cloud APIs for clean audio.

### Memory Usage

**Baseline (no model loaded)**: ~150 MB (Electron + xterm + React)

**With model loaded**:
- tiny.en: +390 MB = ~540 MB total
- base.en: +500 MB = ~650 MB total
- small.en: +1000 MB = ~1150 MB total

**During inference** (additional temporary memory):
- +100-200 MB for audio buffers and processing

**Recommendation**: base.en is acceptable for most systems (< 1 GB total). Offer tiny.en for low-memory systems.

### Cross-Platform Support

| Implementation | Windows | macOS (Intel) | macOS (Apple Silicon) | Linux x64 | Linux ARM |
|----------------|---------|---------------|----------------------|-----------|-----------|
| nodejs-whisper | ‚úÖ CPU/CUDA | ‚úÖ CPU | ‚úÖ CPU | ‚úÖ CPU/CUDA | ‚úÖ CPU |
| @fugood/whisper.node | ‚úÖ CUDA/Vulkan | ‚úÖ CPU | ‚úÖ **Metal** | ‚úÖ CUDA/Vulkan | ‚úÖ Vulkan |
| transformers.js | ‚úÖ WebGPU | ‚úÖ WebGPU | ‚úÖ WebGPU | ‚úÖ WebGPU | ‚úÖ WebGPU |
| smart-whisper | ‚úÖ CPU/GPU | ‚úÖ CPU/GPU | ‚úÖ Auto GPU | ‚úÖ CPU/GPU | ‚úÖ CPU |
| faster-whisper | ‚úÖ CUDA | ‚úÖ CPU | ‚úÖ CPU | ‚úÖ CUDA | ‚ö†Ô∏è CPU |

---

## Performance Benchmarks

### Real-World Usage Scenarios

#### Scenario 1: Quick Terminal Commands (3-5 seconds)
**Goal**: Sub-second transcription for snappy UX

| Implementation | Hardware | Time | Acceptable? |
|----------------|----------|------|-------------|
| nodejs-whisper (CPU) | M1 Mac | ~8 sec | ‚ö†Ô∏è Slow |
| @fugood/whisper.node (Metal) | M1 Mac | ~1.5 sec | ‚úÖ Good |
| nodejs-whisper (CUDA) | RTX 3080 | ~1 sec | ‚úÖ Excellent |
| @fugood/whisper.node (CUDA) | RTX 3080 | ~0.8 sec | ‚úÖ Excellent |

**Verdict**: GPU acceleration is essential for good UX.

#### Scenario 2: Extended Dictation (30-60 seconds)
**Goal**: Faster than typing (WPM > 40)

| Implementation | Hardware | Time | Acceptable? |
|----------------|----------|------|-------------|
| nodejs-whisper (CPU) | M1 Mac | ~180 sec | ‚ùå Too slow |
| @fugood/whisper.node (Metal) | M1 Mac | ~30 sec | ‚úÖ Good |
| nodejs-whisper (CUDA) | RTX 3080 | ~18 sec | ‚úÖ Excellent |
| faster-whisper (CUDA) | RTX 3080 | ~12 sec | ‚úÖ Excellent |

**Verdict**: CPU-only is not viable for extended dictation. GPU required.

#### Scenario 3: Continuous Conversation Mode
**Goal**: Real-time transcription with VAD (Voice Activity Detection)

| Implementation | VAD Support | Streaming | Viable? |
|----------------|-------------|-----------|---------|
| nodejs-whisper | ‚ùå | ‚ùå | ‚ö†Ô∏è Batch only |
| @fugood/whisper.node | ‚ö†Ô∏è External | ‚ùå | ‚ö†Ô∏è Batch only |
| transformers.js | ‚úÖ Via @ricky0123/vad-web | ‚úÖ | ‚úÖ Good |
| faster-whisper | ‚úÖ Built-in Silero VAD | ‚úÖ | ‚úÖ Best |

**Verdict**: For streaming/continuous mode, consider transformers.js + VAD or faster-whisper.

### Network Comparison: Local vs Cloud

**Test**: Transcribe 10-second audio clip

| Method | Time | Cost | Privacy | Offline? |
|--------|------|------|---------|----------|
| Gemini 2.0 Flash | ~2 sec | $0.0001 | ‚ùå Sent to Google | ‚ùå |
| OpenAI Whisper API | ~3 sec | $0.001 | ‚ùå Sent to OpenAI | ‚ùå |
| ElevenLabs Scribe | ~2 sec | $0.0011 | ‚ùå Sent to ElevenLabs | ‚ùå |
| nodejs-whisper (CPU) | ~25 sec | $0 | ‚úÖ Local | ‚úÖ |
| @fugood/whisper.node (Metal) | ~3 sec | $0 | ‚úÖ Local | ‚úÖ |
| faster-whisper (CUDA) | ~2 sec | $0 | ‚úÖ Local | ‚úÖ |

**Verdict**: Cloud is faster and cheaper per-request, but local is better for:
- Privacy-sensitive use cases
- Offline operation
- No recurring API costs
- High-volume usage

---

## Recommended Approach

### Phase 1: Current State (Already Done ‚úÖ)
**Status**: Production-ready for CPU users

**What's working**:
- nodejs-whisper integration in `electron/whisperService.cjs`
- IPC handlers for transcription
- Model selection (tiny.en, base.en, small.en)
- Auto-download on first use
- Fallback to cloud APIs if local fails

**Current user experience**:
- Settings ‚Üí Select "Whisper Local (Base)" from model dropdown
- First use: Model auto-downloads (142 MB)
- Subsequent uses: Transcription takes ~15-30 sec for typical commands
- Works offline

**Limitations**:
- Slow on CPU (not ideal for real-time dictation)
- No GPU acceleration on macOS (Metal)
- No AMD GPU support (Vulkan)

### Phase 2: GPU Acceleration (Recommended Next Step)
**Goal**: Make local transcription fast enough for real-time use

**Implementation**: Add @fugood/whisper.node as optional GPU-accelerated backend

**Steps**:
1. Install dependency: `npm install @fugood/whisper.node`
2. Create new service: `electron/whisperServiceGPU.cjs`
3. Detect GPU capabilities at startup
4. Fall back to nodejs-whisper if GPU unavailable
5. Add UI toggle in Settings: "Use GPU Acceleration (faster)"

**Benefits**:
- 5-10x speedup on Apple Silicon Macs (Metal)
- 8-12x speedup on NVIDIA GPUs (CUDA)
- 3-5x speedup on AMD GPUs (Vulkan)
- Same accuracy as CPU version
- Maintains offline capability

**Code sketch** (see Integration Patterns section below)

### Phase 3: Voice Activity Detection (Future)
**Goal**: Eliminate manual "start/stop recording" for continuous dictation

**Implementation**: Add `@ricky0123/vad-web` for browser-based VAD

**Features**:
- Auto-start transcription when speech detected
- Auto-stop when silence detected
- Batch process speech segments
- Better UX for extended dictation

### Phase 4: Streaming Mode (Advanced)
**Goal**: Real-time transcription during speech (like commercial tools)

**Options**:
- transformers.js + VAD for renderer-based streaming
- Python + faster-whisper for best performance
- Local API server for shared resources

**Tradeoffs**: Significantly more complex, requires major refactor

---

## Integration Patterns

### Current Pattern (nodejs-whisper)

**Already implemented in AudioBash**:

```javascript
// electron/whisperService.cjs
const { nodewhisper } = require('nodejs-whisper');

class WhisperService {
  async transcribe(audioPath) {
    const options = {
      modelName: this.currentModel,
      autoDownloadModelName: this.currentModel,
      whisperOptions: {
        outputInText: true,
        translateToEnglish: false,
        wordTimestamps: false,
      }
    };

    const result = await nodewhisper(audioPath, options);
    return { text: result.trim() };
  }
}
```

**Pros**: Simple, auto-downloads models, works today
**Cons**: CPU-only (no Metal/Vulkan), slow for real-time use

### Recommended Pattern: GPU Acceleration with @fugood/whisper.node

**New service for GPU-accelerated transcription**:

```javascript
// electron/whisperServiceGPU.cjs
const { initWhisper, whisper } = require('@fugood/whisper.node');

class WhisperServiceGPU {
  constructor() {
    this.whisperContext = null;
    this.currentModel = 'base.en';
    this.gpuAvailable = this.detectGPU();
  }

  detectGPU() {
    const os = require('os');
    const platform = os.platform();

    // Metal available on macOS (M1/M2/M3)
    if (platform === 'darwin') {
      return { type: 'metal', available: true };
    }

    // CUDA available on Windows/Linux with NVIDIA GPU
    // Vulkan available on Windows/Linux with AMD GPU
    // TODO: Add GPU detection logic

    return { type: 'cpu', available: false };
  }

  async initialize() {
    const modelPath = this.getModelPath(this.currentModel);

    this.whisperContext = await initWhisper({
      modelPath: modelPath,
      useGpu: this.gpuAvailable.available,
      libVariant: this.gpuAvailable.type === 'metal' ? 'default' :
                  this.gpuAvailable.type === 'cuda' ? 'cuda' :
                  this.gpuAvailable.type === 'vulkan' ? 'vulkan' : 'default',
    });
  }

  async transcribe(audioPath) {
    if (!this.whisperContext) {
      await this.initialize();
    }

    // Convert audio to required format (16-bit PCM, mono, 16kHz)
    const audioData = await this.convertAudio(audioPath);

    const result = await whisper(this.whisperContext, {
      audio: audioData,
      options: {
        language: 'en',
        translate: false,
        max_len: 1,
        token_timestamps: false,
      }
    });

    return { text: result.trim() };
  }

  async convertAudio(audioPath) {
    // Use ffmpeg or sox to convert WebM ‚Üí 16-bit PCM, mono, 16kHz
    // Return ArrayBuffer with raw audio data
    // Implementation depends on available audio library

    const fs = require('fs');
    const { exec } = require('child_process');
    const util = require('util');
    const execPromise = util.promisify(exec);

    const tempPCM = audioPath.replace('.webm', '.pcm');

    // Convert using ffmpeg (must be available on system)
    await execPromise(
      `ffmpeg -i "${audioPath}" -ar 16000 -ac 1 -f s16le "${tempPCM}"`
    );

    const buffer = fs.readFileSync(tempPCM);
    fs.unlinkSync(tempPCM); // Clean up

    return buffer.buffer; // ArrayBuffer
  }
}

module.exports = new WhisperServiceGPU();
```

**Update main.cjs to use GPU service when available**:

```javascript
// electron/main.cjs
const whisperService = require('./whisperService.cjs');
const whisperServiceGPU = require('./whisperServiceGPU.cjs');

// Add GPU detection at startup
let useGPU = false;
app.whenReady().then(() => {
  const gpuInfo = whisperServiceGPU.detectGPU();
  useGPU = gpuInfo.available;
  console.log(`[AudioBash] GPU transcription: ${useGPU ? 'enabled' : 'disabled'}`);
});

ipcMain.handle('whisper-transcribe', async (_, audioPath) => {
  try {
    // Use GPU service if available, otherwise fall back to CPU
    const service = useGPU ? whisperServiceGPU : whisperService;
    const result = await service.transcribe(audioPath);
    return result;
  } catch (err) {
    console.error('[AudioBash] Whisper transcription error:', err);
    // Fall back to CPU on GPU error
    if (useGPU) {
      console.log('[AudioBash] GPU failed, falling back to CPU');
      return await whisperService.transcribe(audioPath);
    }
    return { text: '', error: err.message };
  }
});
```

**Pros**:
- Huge speedup on supported hardware
- Automatic fallback to CPU
- No changes required to renderer code
- Same IPC interface

**Cons**:
- Requires audio format conversion (ffmpeg dependency)
- More complex error handling
- Platform-specific binaries increase bundle size

### Alternative Pattern: Renderer-Based (transformers.js)

**For web version or to avoid IPC overhead**:

```typescript
// src/services/whisperRenderer.ts
import { pipeline } from '@huggingface/transformers';

class WhisperRenderer {
  private transcriber: any = null;

  async initialize() {
    // Load Whisper model in renderer process
    this.transcriber = await pipeline(
      'automatic-speech-recognition',
      'Xenova/whisper-base.en',
      { device: 'webgpu' } // Use WebGPU if available
    );
  }

  async transcribe(audioBlob: Blob): Promise<string> {
    if (!this.transcriber) {
      await this.initialize();
    }

    // Transcribe directly from blob
    const result = await this.transcriber(audioBlob);
    return result.text.trim();
  }
}

export const whisperRenderer = new WhisperRenderer();
```

**Usage in VoicePanel.tsx**:

```typescript
// src/components/VoicePanel.tsx
import { whisperRenderer } from '../services/whisperRenderer';

// In handleStopRecording:
if (modelId.startsWith('whisper-local-')) {
  // Use renderer-based transcription (no IPC)
  const text = await whisperRenderer.transcribe(audioBlob);
  onTranscriptionComplete(text);
} else {
  // Use IPC for other providers
  const result = await transcriptionService.transcribeAudio(audioBlob, mode, modelId);
  onTranscriptionComplete(result.text);
}
```

**Pros**:
- No IPC overhead
- Works in web version
- WebGPU acceleration
- No native dependencies

**Cons**:
- Slower than native implementations
- Larger memory footprint
- WebGPU not supported on all systems
- Model download happens in renderer (larger bundle)

### VAD Integration Pattern (Future Enhancement)

**Add Voice Activity Detection for hands-free mode**:

```typescript
// src/hooks/useVAD.ts
import { useMicVAD } from '@ricky0123/vad-react';

export function useVoiceActivityDetection(
  onSpeechEnd: (audio: Float32Array) => void,
  enabled: boolean
) {
  const vad = useMicVAD({
    startOnLoad: enabled,
    onSpeechEnd: (audio) => {
      console.log('[VAD] Speech ended, transcribing...');
      onSpeechEnd(audio);
    },
    onSpeechStart: () => {
      console.log('[VAD] Speech detected');
    },
    positiveSpeechThreshold: 0.8,
    negativeSpeechThreshold: 0.8 - 0.15,
  });

  return {
    start: vad.start,
    pause: vad.pause,
    listening: vad.listening,
  };
}
```

**Usage**:

```typescript
// src/components/VoicePanel.tsx
const { start, pause, listening } = useVoiceActivityDetection(
  (audio) => {
    // Convert Float32Array to Blob and transcribe
    const audioBlob = float32ToBlob(audio);
    transcribeAudio(audioBlob);
  },
  continuousMode // User preference
);
```

**Benefits**:
- Hands-free dictation
- Automatic silence detection
- Better UX for long-form content
- Works with any transcription backend

---

## Known Limitations

### Current Implementation (nodejs-whisper)

1. **Slow CPU Performance**
   - Typical 5-sec command takes ~15-30 sec to transcribe
   - Not viable for real-time dictation
   - User must wait for results

2. **No GPU Acceleration on macOS**
   - No Metal support (important for M1/M2/M3 Macs)
   - M1 Mac as fast as older Intel Mac

3. **FFmpeg Dependency**
   - Required for audio format conversion
   - Must be installed on system
   - Not bundled with Electron app

4. **No Streaming Support**
   - Batch processing only
   - Must record full audio before transcription
   - Can't show partial results

### General Whisper Limitations

1. **Technical Vocabulary**
   - Struggles with programming terms (e.g., "npm install" ‚Üí "and pm install")
   - Command-line jargon not in training data
   - Workaround: Use custom vocabulary corrections (already implemented)

2. **Background Noise**
   - Accuracy degrades with noise
   - Tiny/base models more affected than large models
   - Workaround: Use VAD to filter non-speech

3. **Accents & Languages**
   - `.en` models optimized for English only
   - Non-native accents may have lower accuracy
   - Workaround: Use multilingual models (tiny, base, small)

4. **Model Download Size**
   - First-time setup requires internet
   - 142 MB (base) to 2.9 GB (large) download
   - Stored in `~/.audiobash/models/` (or nodejs-whisper default location)

5. **Memory Usage**
   - 500 MB - 4.7 GB RAM depending on model
   - Can't use on very low-end systems
   - Workaround: Offer cloud API fallback

### Platform-Specific Issues

**Windows**:
- CUDA requires NVIDIA GPU (no AMD support via nodejs-whisper)
- FFmpeg must be installed separately
- Vulkan support requires @fugood/whisper.node

**macOS**:
- No GPU acceleration with nodejs-whisper
- FFmpeg not bundled (must install via Homebrew)
- Metal support requires @fugood/whisper.node

**Linux**:
- FFmpeg usually available via package manager
- CUDA requires proprietary NVIDIA drivers
- Vulkan support varies by distro

### Comparison to Cloud APIs

**What Local Whisper Can't Match**:

1. **Speed** (on CPU hardware)
   - Gemini 2.0 Flash: ~2 sec
   - Local CPU: ~20-30 sec
   - Gap narrows with GPU (local ~3 sec with CUDA/Metal)

2. **Specialized Models**
   - Cloud APIs use domain-specific tuning
   - Local Whisper is general-purpose
   - ElevenLabs Scribe optimized for voice typing

3. **Continuous Updates**
   - Cloud models improve over time
   - Local models frozen at download
   - Must manually update to new versions

**What Local Whisper Does Better**:

1. **Privacy** - No audio sent to cloud
2. **Offline** - Works without internet
3. **Cost** - Free after initial download
4. **Latency** - No network round-trip (with GPU)

---

## Migration Path

### For Existing AudioBash Users

**Current**: Cloud APIs (Gemini, OpenAI)
**Migration**: Gradual opt-in to local transcription

**Step 1: Inform Users**
- Add banner in Settings: "Try offline transcription (beta)"
- Explain benefits: privacy, offline, free
- Explain tradeoffs: slower on CPU, faster with GPU

**Step 2: One-Click Setup**
- "Enable Local Transcription" button in Settings
- Auto-detect GPU capabilities
- Download recommended model (base.en, 142 MB)
- Test transcription with sample audio

**Step 3: Hybrid Mode** (Recommended)
- Default: Cloud API (fast, reliable)
- Fallback: Local Whisper if offline or cloud API fails
- User toggle: "Prefer Local Transcription"

**Step 4: Monitor Usage**
- Track success rates: local vs cloud
- Log performance metrics (speed, accuracy)
- Collect user feedback

### For New Users

**Onboarding Flow**:

1. **Welcome Screen**
   - "Choose transcription method"
   - Option A: Cloud APIs (requires API key, fast)
   - Option B: Local Whisper (free, slower on CPU)
   - Option C: Both (recommended)

2. **GPU Detection**
   - Auto-detect GPU capabilities
   - "Your system has a [NVIDIA RTX 3080]"
   - "Local transcription will be fast (~3 sec per command)"
   - OR: "Your system uses CPU only"
   - "Local transcription will be slower (~20 sec per command)"

3. **Model Download**
   - "Download base.en model (142 MB)?"
   - Progress bar during download
   - "Model ready for offline use"

4. **First Transcription**
   - "Try it: Say a command"
   - Record ‚Üí Transcribe ‚Üí Show result
   - "Your transcription took X seconds"
   - Option to switch to cloud if too slow

---

## Performance Optimization Tips

### For CPU-Only Users

1. **Use tiny.en model** (75 MB)
   - 2-3x faster than base.en
   - Acceptable accuracy for commands
   - Settings ‚Üí Model ‚Üí "Whisper Local (Tiny)"

2. **Keep recordings short** (< 10 seconds)
   - Speak commands concisely
   - Don't dictate paragraphs
   - Use agent mode for command translation

3. **Batch processing**
   - Record multiple commands
   - Process overnight or during breaks
   - Not ideal for real-time use

4. **Hybrid approach**
   - Use cloud API for extended dictation
   - Use local Whisper for quick commands (when offline)

### For GPU Users

1. **Use base.en or small.en**
   - GPU makes these models fast enough
   - Better accuracy than tiny.en
   - Still < 5 sec processing time

2. **Enable GPU acceleration**
   - Settings ‚Üí "Use GPU Acceleration"
   - Auto-detects Metal/CUDA/Vulkan
   - Falls back to CPU if GPU fails

3. **Close other GPU apps**
   - Free up VRAM for Whisper
   - Games, video editing, mining, etc.

4. **Monitor GPU temperature**
   - Extended use may heat up laptop
   - Consider breaks during long sessions

### For Developers

1. **Lazy load models**
   - Don't load model at startup
   - Load on first transcription request
   - Unload after period of inactivity

2. **Model caching**
   - Keep model in memory between requests
   - Avoid reload overhead
   - Trade memory for speed

3. **Parallel processing** (if using smart-whisper)
   - Process multiple recordings simultaneously
   - Utilize multiple CPU cores
   - Especially useful for batch imports

4. **Quantization** (if using faster-whisper)
   - Use 8-bit quantized models
   - 2x faster with minimal accuracy loss
   - Requires CTranslate2 backend

---

## Cost Analysis

### Cloud APIs (Current)

**Per-Minute Costs**:
- Gemini 2.0 Flash: ~$0.0001/min (cheap!)
- OpenAI Whisper API: $0.006/min
- ElevenLabs Scribe: $0.0067/min

**Monthly Costs** (heavy user: 2 hours/day):
- Gemini: $0.36/month
- OpenAI: $21.60/month
- ElevenLabs: $24.12/month

**Annual Costs**:
- Gemini: $4.32/year ‚ö° Cheapest
- OpenAI: $259.20/year
- ElevenLabs: $289.44/year

### Local Whisper

**One-Time Costs**:
- Model download: Free (internet bandwidth)
- Storage: 75 MB - 2.9 GB (negligible)
- GPU hardware: $0 (use existing) or $200-$1000 (new GPU)

**Ongoing Costs**:
- Electricity: ~$0.01/hour (GPU) or ~$0.002/hour (CPU)
- Monthly (2 hours/day): ~$0.60/month (GPU) or ~$0.12/month (CPU)
- Annual: ~$7.20/year (GPU) or ~$1.44/year (CPU)

**Break-Even Analysis**:
- vs OpenAI Whisper: Immediate (saves $259/year)
- vs ElevenLabs: Immediate (saves $282/year)
- vs Gemini: 8 years (saves $3/year, but privacy/offline benefits)

**Verdict**: Local Whisper is cost-effective if you use transcription frequently or value privacy/offline capability.

---

## Troubleshooting

### Common Issues

#### 1. "Whisper model not found"
**Cause**: Model not downloaded
**Fix**:
- First transcription triggers auto-download (nodejs-whisper)
- Wait for download to complete (142 MB for base.en)
- Check `~/.audiobash/models/` or nodejs-whisper cache location
- Manually download via Settings ‚Üí Download Models

#### 2. "Transcription takes forever" (> 1 minute)
**Cause**: CPU-only processing
**Fix**:
- Check GPU availability in Settings ‚Üí System Info
- Enable GPU acceleration if available
- Use smaller model (tiny.en instead of base.en)
- Consider cloud API for faster results

#### 3. "FFmpeg not found"
**Cause**: Audio conversion dependency missing
**Fix**:
- **Windows**: Download from https://ffmpeg.org/download.html, add to PATH
- **macOS**: `brew install ffmpeg`
- **Linux**: `sudo apt install ffmpeg` (Debian/Ubuntu) or `sudo yum install ffmpeg` (RHEL/CentOS)

#### 4. "GPU acceleration not working"
**Cause**: GPU library variant not installed or incompatible
**Fix**:
- **macOS**: Requires @fugood/whisper.node (not nodejs-whisper)
- **Windows/Linux CUDA**: Check NVIDIA drivers, CUDA toolkit installed
- **Windows/Linux Vulkan**: Check Vulkan runtime installed
- Fall back to CPU mode if GPU unavailable

#### 5. "Out of memory error"
**Cause**: Model too large for available RAM
**Fix**:
- Use smaller model (tiny.en: ~390 MB RAM)
- Close other applications
- Upgrade system RAM
- Use cloud API instead

#### 6. "Transcription is gibberish"
**Cause**: Audio format mismatch or corrupted audio
**Fix**:
- Ensure WebM audio is valid
- Check ffmpeg conversion logs
- Test with known-good audio file
- Verify sample rate (should be 16kHz)

#### 7. "Model download stuck"
**Cause**: Network issues or slow connection
**Fix**:
- Check internet connection
- Retry download
- Manually download from Hugging Face and place in models directory
- Use different mirror if available

---

## Future Enhancements

### Short-Term (Next Release)

1. **GPU Auto-Detection**
   - Detect Metal/CUDA/Vulkan at startup
   - Auto-enable GPU mode if available
   - Show GPU status in Settings

2. **Model Management UI**
   - Download/delete models from Settings
   - Show disk usage per model
   - Clear model cache

3. **Performance Metrics**
   - Show transcription speed (Xx realtime)
   - Log GPU usage during inference
   - Compare local vs cloud speed

### Medium-Term (Q1-Q2 2026)

1. **VAD Integration**
   - Add @ricky0123/vad-web for continuous mode
   - Hands-free dictation
   - Auto-segment long recordings

2. **Streaming Transcription**
   - Show partial results during transcription
   - Progress indicator with estimated time
   - Cancel in-progress transcriptions

3. **Model Quantization**
   - Offer 8-bit quantized models (faster, less accurate)
   - User choice: speed vs accuracy

### Long-Term (2026+)

1. **Python Bridge for faster-whisper**
   - Bundle Python runtime with Electron
   - Integrate CTranslate2 backend
   - 4x speedup over current implementation

2. **Custom Fine-Tuning**
   - Allow users to fine-tune on their own voice
   - Improve accuracy for accents/jargon
   - Export/import custom models

3. **Local API Server Mode**
   - Run AudioBash as transcription server
   - Share across devices/users
   - Wyoming protocol support (Home Assistant)

---

## Conclusion

### Summary of Recommendations

**Current State**: ‚úÖ nodejs-whisper works, but slow on CPU

**Phase 1 (Now)**:
- ‚úÖ Keep nodejs-whisper for CPU users
- ‚úÖ Document performance expectations
- ‚úÖ Recommend cloud APIs for real-time use

**Phase 2 (Next Release - Recommended)**:
- ‚≠ê Add @fugood/whisper.node for GPU acceleration
- ‚≠ê Auto-detect Metal/CUDA/Vulkan
- ‚≠ê Fall back to nodejs-whisper if GPU unavailable
- Target: 5-10x speedup for GPU users

**Phase 3 (Future)**:
- Add VAD for continuous mode
- Add streaming transcription with progress
- Consider Python + faster-whisper for max performance

### Decision Matrix

**Choose nodejs-whisper (current) if**:
- ‚úÖ You want offline transcription NOW
- ‚úÖ You're okay with 15-30 sec latency
- ‚úÖ You use short commands (< 10 sec audio)
- ‚úÖ You have a CPU-only system

**Upgrade to @fugood/whisper.node if**:
- ‚≠ê You have an M1/M2/M3 Mac (Metal GPU)
- ‚≠ê You have an NVIDIA GPU (CUDA)
- ‚≠ê You have an AMD GPU (Vulkan)
- ‚≠ê You want 5-10x faster transcription
- ‚≠ê You dictate frequently (daily use)

**Consider transformers.js if**:
- üåê You want a web version of AudioBash
- üåê You want to avoid native dependencies
- üåê You're okay with slower speed (vs native)

**Consider Python + faster-whisper if**:
- üöÄ You need absolute best performance
- üöÄ You're comfortable with Python dependencies
- üöÄ You have a powerful NVIDIA GPU
- üöÄ You use transcription heavily (> 2 hours/day)

**Stick with cloud APIs if**:
- ‚òÅÔ∏è You need the fastest transcription (< 2 sec)
- ‚òÅÔ∏è You're always online
- ‚òÅÔ∏è You don't mind sending audio to cloud
- ‚òÅÔ∏è You use infrequently (< $5/month cost)

### Final Verdict

**For AudioBash v2.1 (Next Release)**:

1. **Keep nodejs-whisper** as baseline (already working)
2. **Add @fugood/whisper.node** as optional GPU accelerator
3. **Auto-detect GPU** and enable if available
4. **Maintain hybrid approach** (local + cloud)
5. **Document performance** expectations clearly

**Implementation effort**: ~2-3 days for @fugood/whisper.node integration
**User impact**: High (5-10x speedup for GPU users, no change for CPU users)
**Risk**: Low (fallback to existing nodejs-whisper if GPU fails)

---

## References

### Documentation
- [nodejs-whisper npm](https://www.npmjs.com/package/nodejs-whisper)
- [@fugood/whisper.node npm](https://www.npmjs.com/package/@fugood/whisper.node)
- [Transformers.js docs](https://huggingface.co/docs/transformers.js/en/index)
- [whisper.cpp GitHub](https://github.com/ggml-org/whisper.cpp)
- [faster-whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [smart-whisper npm](https://www.npmjs.com/package/smart-whisper)

### Performance Benchmarks
- [Modal: 5 Ways to Speed Up Whisper](https://modal.com/blog/faster-transcription)
- [Modal: Choosing Whisper Variants](https://modal.com/blog/choosing-whisper-variants)
- [Tom's Hardware: Whisper GPU Benchmarks](https://www.tomshardware.com/news/whisper-audio-transcription-gpus-benchmarked)
- [AssemblyAI: Offline Whisper Comparison](https://www.assemblyai.com/blog/offline-speech-recognition-whisper-browser-node-js)

### Integration Guides
- [Leapcell: Whisper in Node.js](https://leapcell.io/blog/how-to-run-whisper-in-nodejs-with-word-level-timestamp)
- [DEV: Transformers.js Speech-to-Text](https://dev.to/emojiiii/how-to-build-a-speech-to-text-app-with-react-and-transformersjs-4n1f)
- [GitHub: transformersjs-electron](https://github.com/Mintplex-Labs/transformersjs-electron)

### AudioBash Files
- `/home/user/audiobash/electron/whisperService.cjs` - Current implementation
- `/home/user/audiobash/src/services/transcriptionService.ts` - Multi-provider service
- `/home/user/audiobash/docs/VOICE_DICTATION_RESEARCH.md` - Prior research

---

**Last Updated**: January 7, 2026
**Author**: Research compiled for AudioBash local transcription
**Status**: Production recommendations for v2.1 release
